{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T20:29:21.551455Z",
     "start_time": "2020-02-07T20:29:19.449241Z"
    },
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cbrUtil.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import (Activation, \n",
    "                          Convolution2D, \n",
    "                          Dense, \n",
    "                          Flatten, \n",
    "                          Permute)\n",
    "from keras.models import (Sequential, \n",
    "                          load_model)\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import (FileLogger, \n",
    "                          ModelIntervalCheckpoint, \n",
    "                          WandbLogger)\n",
    "from rl.core import Processor\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import (EpsGreedyQPolicy, \n",
    "                       LinearAnnealedPolicy)\n",
    "\n",
    "import import_ipynb\n",
    "from cbrUtil import AtariProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build TrainAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T20:29:21.672776Z",
     "start_time": "2020-02-07T20:29:21.625556Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainAgent():\n",
    "    \"\"\"\n",
    "    This class implements the \"Training Agent\" of the proposed PENG \n",
    "    Architecture.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    env_name : str\n",
    "        exact name of the Atari environment.\n",
    "    name_of_run : str\n",
    "        description of the current run will be used in WandbLogger.\n",
    "    transferMode : boolean\n",
    "        true=agent learns from previous model. \n",
    "        false=agent learns from scratch.\n",
    "    transfer_architecture : str\n",
    "        path to the architecture to learn from.\n",
    "    transfer_policy : str\n",
    "        path to the policy that is injected into the architecture.\n",
    "    transfer_game_nb_actions : int\n",
    "        number of actions from transfer game\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    printInitialization(self):\n",
    "        Prints the initial values.\n",
    "    \n",
    "    buildModelBaseline(self):\n",
    "        builds the keras model if no transfer happens.\n",
    "    \n",
    "    buildModelInjection(self):\n",
    "        builds the keras model if transfer happens.\n",
    "    \n",
    "    configAgentInjection(self):\n",
    "        configs the transfer agent model.\n",
    "    \n",
    "    compileAgentInjection(self):\n",
    "        compiles the transfer agent.\n",
    "    \n",
    "    getModelBaselineSummary(self):\n",
    "        returns keras model summary.\n",
    "    \n",
    "    getModelInjectionSummary(self):\n",
    "        returns keras model summary for transfer.\n",
    "    \n",
    "    configAgentBaseline(self):\n",
    "        configs the agent model.\n",
    "    \n",
    "    compileAgentBaseline(self):\n",
    "        compiles the agent.\n",
    "    \n",
    "    createCallbacks(self):\n",
    "        creates keras-rl callbacks.\n",
    "    \n",
    "    trainingBaseline(self):\n",
    "        start agent training.\n",
    "    \n",
    "    trainingInjection(self):\n",
    "        start transfer agent training.\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    def __init__(self,\n",
    "                 env_name,\n",
    "                 name_of_run,\n",
    "                 transferMode = False,\n",
    "                 transfer_architecture = None,\n",
    "                 transfer_policy = None,\n",
    "                 transfer_game_nb_actions = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the environment.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        env_name : str\n",
    "            exact name of the Atari environment.\n",
    "        name_of_run : str\n",
    "            description of the current run will be used in WandbLogger.\n",
    "        transferMode : boolean\n",
    "            true=agent learns from previous model. \n",
    "            false=agent learns from scratch.\n",
    "        transfer_architecture : str\n",
    "            path to the architecture to learn from.\n",
    "        transfer_policy : str\n",
    "            path to the policy that is injected into the architecture.\n",
    "        transfer_game_nb_actions : int\n",
    "            number of actions from transfer game\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.INPUT_SHAPE = (84,84)\n",
    "        self.WINDOW_LENGTH = 4\n",
    "        \n",
    "        #init gym environment with random seed for reproduction\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        np.random.seed(123)\n",
    "        self.env.seed(123)\n",
    "        \n",
    "        \n",
    "        #get nb_actions for the learning env\n",
    "        self.nb_actions = self.env.action_space.n\n",
    "        \n",
    "        #init model, memory, processor, policy, dqn and history\n",
    "        #same for baseline and transfer\n",
    "        self.model = None\n",
    "        self.memory = None\n",
    "        self.processor = None\n",
    "        self.policy = None\n",
    "        self.dqn = None\n",
    "        self.history = None\n",
    "        \n",
    "        #model to be trained when transfermode = true\n",
    "        self.injectionModel = None #model to be injected into training\n",
    "        \n",
    "        #true->agent learns from similar model\n",
    "        #false->agent learns from scratch\n",
    "        self.transferMode = transferMode\n",
    "        \n",
    "        #defines the path to the ModelRepo of the environment  \n",
    "        self.path = './KC/ModelRepo/Atari_'+ self.env_name\n",
    "        self.path_to_architecture = self.path+'/Architecture/'\n",
    "        self.path_to_policy = self.path+'/Policy/'\n",
    "        \n",
    "        #bring some uniqueness to the saving name\n",
    "        self.TIME = str(int(time.time()))\n",
    "        self.SAVE_NAME = self.env_name+'_'+self.TIME\n",
    "        self.name_of_run = (self.env_name+\n",
    "                            '_'+name_of_run+\n",
    "                            '_'+self.TIME)\n",
    "        \n",
    "        #for building the keras-rl callbacks\n",
    "        self.policy_filename = None\n",
    "        self.ckpnt_policy_filename = None\n",
    "        self.architecture_filename_start = None\n",
    "        self.architecture_filename_end = None\n",
    "        self.callbacks=None\n",
    "        \n",
    "        #print environment inits\n",
    "        self.printInitialization()\n",
    "        \n",
    "        if self.transferMode == False:\n",
    "            print(\"Starting normal training mode.\")\n",
    "            self.buildModelBaseline()\n",
    "            self.configAgentBaseline()\n",
    "            self.compileAgentBaseline()\n",
    "            self.createCallbacks() #same in all scenarios\n",
    "        else:\n",
    "            print(\"Starting transfer training mode.\")\n",
    "            #most similar model = architecture+policy\n",
    "            \n",
    "            self.transfer_architecture = load_model(\n",
    "                                                transfer_architecture)\n",
    "            \n",
    "            self.transfer_policy = transfer_policy\n",
    "\n",
    "            #nb_actions of most similar model\n",
    "            self.transfer_game_nb_actions = transfer_game_nb_actions\n",
    "            \n",
    "            assert self.transfer_architecture is not None\n",
    "            assert self.transfer_policy is not None\n",
    "            assert self.transfer_game_nb_actions is not None\n",
    "            \n",
    "            self.buildModelInjection()\n",
    "            self.configAgentInjection()\n",
    "            self.compileAgentInjection()\n",
    "            self.createCallbacks()\n",
    "            \n",
    "        \n",
    "    def printInitialization(self):\n",
    "        \"\"\"\n",
    "        Prints the initial values.\n",
    "        \"\"\"\n",
    "        print(f\"Env:{self.env_name}\\n\"+\n",
    "              f\"nb_actions:{self.nb_actions}\\n\"+\n",
    "              f\"Path:{self.path}\\n\"+\n",
    "              f\"Name of Run: {self.name_of_run}\\n\"+\n",
    "              f\"Save Name: {self.SAVE_NAME}\")\n",
    "    \n",
    "    def buildModelBaseline(self):\n",
    "        \"\"\"\n",
    "        builds the keras model if no transfer happens.\n",
    "        \"\"\"\n",
    "        input_shape = (self.WINDOW_LENGTH,) + self.INPUT_SHAPE\n",
    "        model = Sequential()\n",
    "        if K.common.image_dim_ordering() == 'tf':\n",
    "            # (width, height, channels)\n",
    "            model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "        elif K.image_dim_ordering() == 'th':\n",
    "            # (channels, width, height)\n",
    "            model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "        else:\n",
    "            raise RuntimeError('Unknown image_dim_ordering.')\n",
    "        model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.nb_actions))\n",
    "        model.add(Activation('linear'))\n",
    "        self.model = model\n",
    "        #print(model.summary())\n",
    "        \n",
    "    def buildModelInjection(self):\n",
    "        \"\"\"\n",
    "        builds the keras model if transfer happens.\n",
    "        \"\"\"\n",
    "        #model with same nb_actions: -> just insert the weights\n",
    "        if self.transfer_game_nb_actions==self.nb_actions:\n",
    "            print(\"Same numb actions\")\n",
    "            self.buildModelBaseline()\n",
    "            self.transfer_architecture = self.model\n",
    "            self.transfer_architecture.load_weights(\n",
    "                                                self.transfer_policy)\n",
    "            \n",
    "            self.injectionModel = self.transfer_architecture\n",
    "        else:\n",
    "            \n",
    "            newModel = Sequential()\n",
    "            self.transfer_architecture.load_weights(\n",
    "                                                self.transfer_policy)\n",
    "            \n",
    "            #transformArchitecture\n",
    "            for layer in self.transfer_architecture.layers[:-2]:\n",
    "                newModel.add(layer)\n",
    "            \n",
    "            newModel.add(Dense(units=self.nb_actions,\n",
    "                               kernel_initializer='random_uniform',\n",
    "                               name='dense_'+self.TIME))\n",
    "            \n",
    "            newModel.add(Activation('linear',\n",
    "                                    name='activation_'+self.TIME))\n",
    "            \n",
    "            #changeLastLayerWeights - for testing purpose\n",
    "            #otherwise uncomment random_weights = ....\n",
    "            rando = [0.6130022623869411,\n",
    "                      0.5847831013889547,\n",
    "                      0.5576423909034692,\n",
    "                      0.5527961438581136,\n",
    "                      0.6381476456945895,\n",
    "                      0.6665673062642212,\n",
    "                      0.6184783155260986,\n",
    "                      0.5933045164230435,\n",
    "                      0.6347319971692273,\n",
    "                      0.578295870829989,\n",
    "                      0.6404208736445819,\n",
    "                      0.6931395285466848,\n",
    "                      0.5181019754212713,\n",
    "                      0.5116250271400286,\n",
    "                      0.6176034837902878,\n",
    "                      0.650234751059707,\n",
    "                      0.6022634782996388,\n",
    "                      0.684100237641685]\n",
    "            \n",
    "            \n",
    "            weights = newModel.get_weights()\n",
    "            ia=0\n",
    "            for weight in range(len(weights[-1])):\n",
    "                #random_weight = np.random.uniform(low=0.5, high=0.7)\n",
    "                random_weight = rando[ia]\n",
    "                weights[-1][weight] = random_weight\n",
    "                ia+=1\n",
    "            newModel.set_weights(weights)\n",
    "            self.injectionModel = newModel\n",
    "            #self.injectionModel.summary()\n",
    "            \n",
    "    def configAgentInjection(self):\n",
    "        \"\"\"\n",
    "        configs the transfer agent model.\n",
    "        \"\"\"\n",
    "        self.memory = SequentialMemory(limit=1000000, \n",
    "                                  window_length=self.WINDOW_LENGTH)\n",
    "        \n",
    "        self.processor = AtariProcessor()\n",
    "        \n",
    "        #eps=0.5 worked so far for model injection = value_max\n",
    "        self.policy = LinearAnnealedPolicy(EpsGreedyQPolicy(action_size=self.nb_actions,\n",
    "                                                            q_Injection = False), \n",
    "                                      attr='eps', \n",
    "                                      value_max=0.5, \n",
    "                                      value_min=.1, \n",
    "                                      value_test=.05,\n",
    "                                      nb_steps=1000000)\n",
    "        \n",
    "    def compileAgentInjection(self):\n",
    "        \"\"\"\n",
    "        compiles the transfer agent.\n",
    "        \"\"\"\n",
    "    \n",
    "        #intelligent adaptation of the learning rate could be \n",
    "        #beneficial\n",
    "        \n",
    "        self.dqn = DQNAgent(model=self.injectionModel, \n",
    "                            nb_actions=self.nb_actions, \n",
    "                            policy=self.policy, \n",
    "                            memory=self.memory,\n",
    "                            processor=self.processor, \n",
    "                            nb_steps_warmup=50000, \n",
    "                            gamma=.99, \n",
    "                            target_model_update=10000,\n",
    "                            train_interval=4, delta_clip=1.)\n",
    "        \n",
    "        self.dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "    \n",
    "    @property\n",
    "    def getModelBaselineSummary(self):\n",
    "        \"\"\"\n",
    "        returns keras model summary.\n",
    "        \"\"\"\n",
    "        return self.model.summary()\n",
    "    \n",
    "    @property\n",
    "    def getModelInjectionSummary(self):\n",
    "        \"\"\"\n",
    "        returns keras model summary for transfer.\n",
    "        \"\"\"\n",
    "        return self.injectionModel.summary()\n",
    "    \n",
    "    def configAgentBaseline(self):\n",
    "        \"\"\"\n",
    "        configs the agent model.\n",
    "        \"\"\"\n",
    "        self.memory = SequentialMemory(limit=1000000, \n",
    "                                  window_length=self.WINDOW_LENGTH)\n",
    "        \n",
    "        self.processor = AtariProcessor()\n",
    "        \n",
    "        self.policy = LinearAnnealedPolicy(EpsGreedyQPolicy(action_size=self.nb_actions,\n",
    "                                                            q_Injection = True), \n",
    "                                      attr='eps', \n",
    "                                      value_max=1., \n",
    "                                      value_min=.1, \n",
    "                                      value_test=.05,\n",
    "                                      nb_steps=1000000)\n",
    "    def compileAgentBaseline(self):\n",
    "        \"\"\"\n",
    "        compiles the agent model.\n",
    "        \"\"\"\n",
    "        self.dqn = DQNAgent(model=self.model, \n",
    "                            nb_actions=self.nb_actions, \n",
    "                            policy=self.policy, \n",
    "                            memory=self.memory,\n",
    "                            processor=self.processor, \n",
    "                            nb_steps_warmup=50000, \n",
    "                            gamma=.99, \n",
    "                            target_model_update=10000,\n",
    "                            train_interval=4, delta_clip=1.)\n",
    "\n",
    "        self.dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "        \n",
    "    def createCallbacks(self):\n",
    "        \"\"\"\n",
    "        creates keras-rl callbacks.\n",
    "        \"\"\"\n",
    "        self.policy_filename = (self.path_to_policy +\n",
    "                        '{}_weights_END.h5f'.format(self.SAVE_NAME))\n",
    "        \n",
    "        self.ckpnt_policy_filename = (self.path_to_policy+\n",
    "                                       self.SAVE_NAME+\n",
    "                                       '_weights_{step}.h5f')\n",
    "        \n",
    "        \n",
    "        self.architecture_filename_start =(self.path_to_architecture+ \n",
    "                                       f'{self.SAVE_NAME}_start.h5')\n",
    "        \n",
    "                            \n",
    "        self.architecture_filename_end = (self.path_to_architecture + \n",
    "                                '{}_end.h5'.format(self.SAVE_NAME))\n",
    "                            \n",
    "        self.callbacks=[ModelIntervalCheckpoint(\n",
    "                                        self.ckpnt_policy_filename,\n",
    "                                             interval=20000)]\n",
    "\n",
    "        self.callbacks += [WandbLogger(self.name_of_run)]\n",
    "        \n",
    "      \n",
    "    def trainingBaseline(self):\n",
    "        \"\"\"\n",
    "        start agent training.\n",
    "        \"\"\"\n",
    "        self.dqn.save(self.architecture_filename_start)\n",
    "        \n",
    "        self.history = self.dqn.fit(self.env, \n",
    "                               callbacks=self.callbacks, \n",
    "                               nb_steps=2000000, \n",
    "                               log_interval=10000)\n",
    "\n",
    "\n",
    "        self.dqn.save_weights(self.policy_filename, \n",
    "                         overwrite=True)\n",
    "                                \n",
    "        self.dqn.save(self.architecture_filename_end)\n",
    "        \n",
    "    def trainingInjection(self):\n",
    "        \"\"\"\n",
    "        start transfer agent training.\n",
    "        \"\"\"\n",
    "        \n",
    "        #try to use different stopping criterion\n",
    "        \n",
    "        self.dqn.save(self.architecture_filename_start)\n",
    "        \n",
    "        self.history = self.dqn.fit(self.env, \n",
    "                               callbacks=self.callbacks, \n",
    "                               nb_steps=2000000, \n",
    "                               log_interval=10000)\n",
    "\n",
    "\n",
    "        self.dqn.save_weights(self.policy_filename, \n",
    "                         overwrite=True)\n",
    "                                \n",
    "        self.dqn.save(self.architecture_filename_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training...\n",
    "The next code cells show, how to use this notebook standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T20:29:22.406415Z",
     "start_time": "2020-02-07T20:29:22.401737Z"
    }
   },
   "outputs": [],
   "source": [
    "#Which other game is going to be injected\n",
    "INJECT = 'MsPacman-v0'\n",
    "\n",
    "#Name of training environment -> has to be exact\n",
    "ENV_NAME = 'Seaquest-v0' #with ideal q-value range\n",
    "\n",
    "#If model injection is performed (=found a similar game) \n",
    "#than transfer_mode = True\n",
    "TRANSFER_MODE = False\n",
    "\n",
    "#Name of run appears in WandbLogger\n",
    "if TRANSFER_MODE:\n",
    "    NAME_OF_RUN = f'Inject({INJECT})'\n",
    "else:\n",
    "    #Baseline or Q-Inject\n",
    "    NAME_OF_RUN ='Q-Inject'\n",
    "\n",
    "#Define underlying architecture that training should \n",
    "#use (most sim game)\n",
    "TRANSFER_ARCHITECTURE = './KC/ModelRepo/Atari_MsPacman-v0/Architecture/MsPacman-v0_1579634126_end.h5'\n",
    "\n",
    "#Define most succesful policy corresponding to the architecture\n",
    "TRANSFER_POLICY = './KC/ModelRepo/Atari_MsPacman-v0/Policy/MsPacman-v0_1579119635_weights_1840000.h5f'\n",
    "\n",
    "\n",
    "#nb_action of most similar game\n",
    "TRANSFER_GAME_NB_ACTIONS = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T20:29:28.515869Z",
     "start_time": "2020-02-07T20:29:23.141845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env:Seaquest-v0\n",
      "nb_actions:18\n",
      "Path:./KC/ModelRepo/Atari_Seaquest-v0\n",
      "Name of Run: Seaquest-v0_Q-Inject_1581107363\n",
      "Save Name: Seaquest-v0_1581107363\n",
      "Starting normal training mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/mheinz02/DQN\" target=\"_blank\">https://app.wandb.ai/mheinz02/DQN</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/mheinz02/DQN/runs/h4ie3bvf\" target=\"_blank\">https://app.wandb.ai/mheinz02/DQN/runs/h4ie3bvf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize trainAgent\n",
    "trainAgent = TrainAgent(ENV_NAME,\n",
    "                        NAME_OF_RUN,\n",
    "                        transferMode=TRANSFER_MODE,\n",
    "                        transfer_architecture=TRANSFER_ARCHITECTURE,\n",
    "                        transfer_policy=TRANSFER_POLICY,\n",
    "                        transfer_game_nb_actions=TRANSFER_GAME_NB_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-08T08:32:38.034090Z",
     "start_time": "2020-02-07T20:29:28.574226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Learning\n",
      "Saved model\n",
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0012\n",
      "18 episodes - episode_reward: 0.611 [0.000, 3.000] - ale.lives: 2.525\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 28s 3ms/step - reward: 0.0052\n",
      "13 episodes - episode_reward: 3.692 [1.000, 9.000] - ale.lives: 2.425\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 30s 3ms/step - reward: 0.0047\n",
      "15 episodes - episode_reward: 3.467 [1.000, 6.000] - ale.lives: 2.500\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 28s 3ms/step - reward: 0.0085\n",
      "12 episodes - episode_reward: 7.083 [1.000, 17.000] - ale.lives: 2.618\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 29s 3ms/step - reward: 0.0048\n",
      "14 episodes - episode_reward: 3.357 [0.000, 9.000] - ale.lives: 2.470\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: 0.0023\n",
      "18 episodes - episode_reward: 1.222 [0.000, 4.000] - loss: 0.002 - mae: 0.008 - mean_q: 0.016 - mean_eps: 0.951 - ale.lives: 2.477\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 247s 25ms/step - reward: 0.0051\n",
      "14 episodes - episode_reward: 3.214 [0.000, 8.000] - loss: 0.002 - mae: 0.006 - mean_q: 0.013 - mean_eps: 0.942 - ale.lives: 2.470\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0062\n",
      "11 episodes - episode_reward: 6.364 [2.000, 10.000] - loss: 0.002 - mae: 0.011 - mean_q: 0.018 - mean_eps: 0.933 - ale.lives: 2.411\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0019\n",
      "17 episodes - episode_reward: 1.059 [0.000, 6.000] - loss: 0.002 - mae: 0.017 - mean_q: 0.022 - mean_eps: 0.924 - ale.lives: 2.580\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0021\n",
      "19 episodes - episode_reward: 1.158 [0.000, 6.000] - loss: 0.002 - mae: 0.024 - mean_q: 0.032 - mean_eps: 0.915 - ale.lives: 2.391\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0026\n",
      "17 episodes - episode_reward: 1.471 [0.000, 5.000] - loss: 0.002 - mae: 0.020 - mean_q: 0.029 - mean_eps: 0.906 - ale.lives: 2.519\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0017\n",
      "18 episodes - episode_reward: 0.667 [0.000, 5.000] - loss: 0.002 - mae: 0.029 - mean_q: 0.037 - mean_eps: 0.897 - ale.lives: 2.422\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 233s 23ms/step - reward: 0.0026\n",
      "18 episodes - episode_reward: 1.778 [0.000, 6.000] - loss: 0.002 - mae: 0.032 - mean_q: 0.040 - mean_eps: 0.888 - ale.lives: 2.433\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0014\n",
      "17 episodes - episode_reward: 0.765 [0.000, 3.000] - loss: 0.002 - mae: 0.035 - mean_q: 0.045 - mean_eps: 0.879 - ale.lives: 2.545\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0020\n",
      "18 episodes - episode_reward: 1.056 [0.000, 4.000] - loss: 0.002 - mae: 0.044 - mean_q: 0.055 - mean_eps: 0.870 - ale.lives: 2.531\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0096\n",
      "9 episodes - episode_reward: 10.556 [2.000, 27.000] - loss: 0.002 - mae: 0.051 - mean_q: 0.064 - mean_eps: 0.861 - ale.lives: 2.482\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: 0.0068\n",
      "11 episodes - episode_reward: 6.455 [0.000, 10.000] - loss: 0.002 - mae: 0.058 - mean_q: 0.071 - mean_eps: 0.852 - ale.lives: 2.507\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0031\n",
      "18 episodes - episode_reward: 1.444 [0.000, 4.000] - loss: 0.002 - mae: 0.066 - mean_q: 0.082 - mean_eps: 0.843 - ale.lives: 2.634\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0077\n",
      "12 episodes - episode_reward: 6.583 [1.000, 13.000] - loss: 0.002 - mae: 0.078 - mean_q: 0.095 - mean_eps: 0.834 - ale.lives: 2.412\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0117\n",
      "8 episodes - episode_reward: 15.000 [10.000, 19.000] - loss: 0.002 - mae: 0.094 - mean_q: 0.114 - mean_eps: 0.825 - ale.lives: 2.571\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0080\n",
      "11 episodes - episode_reward: 7.273 [3.000, 12.000] - loss: 0.002 - mae: 0.105 - mean_q: 0.129 - mean_eps: 0.816 - ale.lives: 2.473\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 239s 24ms/step - reward: 0.0028\n",
      "19 episodes - episode_reward: 1.368 [0.000, 4.000] - loss: 0.003 - mae: 0.114 - mean_q: 0.141 - mean_eps: 0.807 - ale.lives: 2.533\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0096\n",
      "9 episodes - episode_reward: 10.111 [3.000, 14.000] - loss: 0.003 - mae: 0.124 - mean_q: 0.155 - mean_eps: 0.798 - ale.lives: 2.456\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0110\n",
      "9 episodes - episode_reward: 13.000 [7.000, 25.000] - loss: 0.003 - mae: 0.144 - mean_q: 0.178 - mean_eps: 0.789 - ale.lives: 2.473\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 243s 24ms/step - reward: 0.0024\n",
      "20 episodes - episode_reward: 1.050 [0.000, 5.000] - loss: 0.003 - mae: 0.165 - mean_q: 0.202 - mean_eps: 0.780 - ale.lives: 2.499\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: 0.0103\n",
      "9 episodes - episode_reward: 10.111 [2.000, 20.000] - loss: 0.003 - mae: 0.187 - mean_q: 0.225 - mean_eps: 0.771 - ale.lives: 2.449\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0094\n",
      "10 episodes - episode_reward: 10.900 [6.000, 18.000] - loss: 0.003 - mae: 0.219 - mean_q: 0.262 - mean_eps: 0.762 - ale.lives: 2.620\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0092\n",
      "10 episodes - episode_reward: 8.700 [2.000, 16.000] - loss: 0.003 - mae: 0.246 - mean_q: 0.292 - mean_eps: 0.753 - ale.lives: 2.523\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: 0.0093\n",
      "11 episodes - episode_reward: 8.727 [2.000, 14.000] - loss: 0.003 - mae: 0.274 - mean_q: 0.322 - mean_eps: 0.744 - ale.lives: 2.477\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 242s 24ms/step - reward: 0.0039\n",
      "18 episodes - episode_reward: 1.944 [0.000, 4.000] - loss: 0.003 - mae: 0.297 - mean_q: 0.347 - mean_eps: 0.735 - ale.lives: 2.455\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0027\n",
      "20 episodes - episode_reward: 1.350 [0.000, 6.000] - loss: 0.003 - mae: 0.325 - mean_q: 0.378 - mean_eps: 0.726 - ale.lives: 2.521\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0085\n",
      "11 episodes - episode_reward: 8.000 [1.000, 12.000] - loss: 0.003 - mae: 0.343 - mean_q: 0.396 - mean_eps: 0.717 - ale.lives: 2.609\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0118\n",
      "9 episodes - episode_reward: 12.556 [7.000, 17.000] - loss: 0.003 - mae: 0.363 - mean_q: 0.417 - mean_eps: 0.708 - ale.lives: 2.486\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0112\n",
      "8 episodes - episode_reward: 13.750 [8.000, 22.000] - loss: 0.003 - mae: 0.385 - mean_q: 0.441 - mean_eps: 0.699 - ale.lives: 2.537\n",
      "\n",
      "Interval 35 (340000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0097\n",
      "7 episodes - episode_reward: 14.286 [4.000, 22.000] - loss: 0.003 - mae: 0.424 - mean_q: 0.484 - mean_eps: 0.690 - ale.lives: 2.428\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0043\n",
      "18 episodes - episode_reward: 2.500 [0.000, 7.000] - loss: 0.003 - mae: 0.436 - mean_q: 0.498 - mean_eps: 0.681 - ale.lives: 2.507\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 206s 21ms/step - reward: 0.0102\n",
      "6 episodes - episode_reward: 16.167 [6.000, 25.000] - loss: 0.003 - mae: 0.469 - mean_q: 0.536 - mean_eps: 0.672 - ale.lives: 2.410\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0121\n",
      "8 episodes - episode_reward: 15.625 [8.000, 22.000] - loss: 0.004 - mae: 0.487 - mean_q: 0.553 - mean_eps: 0.663 - ale.lives: 2.611\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 245s 25ms/step - reward: 0.0111\n",
      "9 episodes - episode_reward: 12.444 [4.000, 18.000] - loss: 0.003 - mae: 0.510 - mean_q: 0.577 - mean_eps: 0.654 - ale.lives: 2.475\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 242s 24ms/step - reward: 0.0113\n",
      "9 episodes - episode_reward: 12.889 [4.000, 19.000] - loss: 0.004 - mae: 0.538 - mean_q: 0.608 - mean_eps: 0.645 - ale.lives: 2.498\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0113\n",
      "8 episodes - episode_reward: 12.750 [8.000, 19.000] - loss: 0.004 - mae: 0.558 - mean_q: 0.629 - mean_eps: 0.636 - ale.lives: 2.331\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0124\n",
      "7 episodes - episode_reward: 19.000 [15.000, 23.000] - loss: 0.004 - mae: 0.592 - mean_q: 0.666 - mean_eps: 0.627 - ale.lives: 2.378\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0044\n",
      "20 episodes - episode_reward: 2.400 [0.000, 8.000] - loss: 0.004 - mae: 0.614 - mean_q: 0.688 - mean_eps: 0.618 - ale.lives: 2.389\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0061\n",
      "15 episodes - episode_reward: 3.800 [0.000, 6.000] - loss: 0.004 - mae: 0.644 - mean_q: 0.723 - mean_eps: 0.609 - ale.lives: 2.507\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0104\n",
      "8 episodes - episode_reward: 13.125 [6.000, 20.000] - loss: 0.004 - mae: 0.661 - mean_q: 0.740 - mean_eps: 0.600 - ale.lives: 2.612\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0030\n",
      "21 episodes - episode_reward: 1.571 [0.000, 8.000] - loss: 0.004 - mae: 0.692 - mean_q: 0.776 - mean_eps: 0.591 - ale.lives: 2.453\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0098\n",
      "8 episodes - episode_reward: 12.000 [8.000, 17.000] - loss: 0.004 - mae: 0.720 - mean_q: 0.806 - mean_eps: 0.582 - ale.lives: 2.536\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0054\n",
      "15 episodes - episode_reward: 3.467 [0.000, 15.000] - loss: 0.004 - mae: 0.737 - mean_q: 0.822 - mean_eps: 0.573 - ale.lives: 2.574\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0118\n",
      "9 episodes - episode_reward: 12.333 [6.000, 24.000] - loss: 0.004 - mae: 0.752 - mean_q: 0.838 - mean_eps: 0.564 - ale.lives: 2.501\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0117\n",
      "8 episodes - episode_reward: 15.500 [10.000, 21.000] - loss: 0.004 - mae: 0.778 - mean_q: 0.865 - mean_eps: 0.555 - ale.lives: 2.367\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0107\n",
      "9 episodes - episode_reward: 12.000 [8.000, 23.000] - loss: 0.004 - mae: 0.807 - mean_q: 0.896 - mean_eps: 0.546 - ale.lives: 2.527\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0112\n",
      "9 episodes - episode_reward: 12.333 [7.000, 18.000] - loss: 0.004 - mae: 0.844 - mean_q: 0.937 - mean_eps: 0.537 - ale.lives: 2.260\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0114\n",
      "8 episodes - episode_reward: 14.375 [9.000, 21.000] - loss: 0.004 - mae: 0.883 - mean_q: 0.978 - mean_eps: 0.528 - ale.lives: 2.702\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0104\n",
      "8 episodes - episode_reward: 12.875 [6.000, 19.000] - loss: 0.004 - mae: 0.927 - mean_q: 1.025 - mean_eps: 0.519 - ale.lives: 2.604\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0122\n",
      "7 episodes - episode_reward: 16.571 [8.000, 31.000] - loss: 0.004 - mae: 0.959 - mean_q: 1.057 - mean_eps: 0.510 - ale.lives: 2.423\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0105\n",
      "8 episodes - episode_reward: 12.875 [8.000, 17.000] - loss: 0.005 - mae: 0.996 - mean_q: 1.097 - mean_eps: 0.501 - ale.lives: 2.506\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0115\n",
      "8 episodes - episode_reward: 14.500 [10.000, 20.000] - loss: 0.005 - mae: 1.013 - mean_q: 1.116 - mean_eps: 0.492 - ale.lives: 2.620\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0116\n",
      "8 episodes - episode_reward: 14.875 [9.000, 20.000] - loss: 0.005 - mae: 1.057 - mean_q: 1.164 - mean_eps: 0.483 - ale.lives: 2.566\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0123\n",
      "7 episodes - episode_reward: 17.571 [10.000, 29.000] - loss: 0.005 - mae: 1.087 - mean_q: 1.199 - mean_eps: 0.474 - ale.lives: 2.451\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0121\n",
      "8 episodes - episode_reward: 14.875 [9.000, 30.000] - loss: 0.005 - mae: 1.121 - mean_q: 1.234 - mean_eps: 0.465 - ale.lives: 2.457\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0121\n",
      "6 episodes - episode_reward: 19.167 [12.000, 24.000] - loss: 0.005 - mae: 1.151 - mean_q: 1.268 - mean_eps: 0.456 - ale.lives: 2.580\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0122\n",
      "8 episodes - episode_reward: 16.875 [13.000, 21.000] - loss: 0.005 - mae: 1.184 - mean_q: 1.302 - mean_eps: 0.447 - ale.lives: 2.435\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0119\n",
      "7 episodes - episode_reward: 17.143 [9.000, 23.000] - loss: 0.005 - mae: 1.225 - mean_q: 1.348 - mean_eps: 0.438 - ale.lives: 2.428\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0119\n",
      "7 episodes - episode_reward: 16.429 [12.000, 24.000] - loss: 0.005 - mae: 1.259 - mean_q: 1.384 - mean_eps: 0.429 - ale.lives: 2.583\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0102\n",
      "8 episodes - episode_reward: 13.500 [8.000, 21.000] - loss: 0.005 - mae: 1.261 - mean_q: 1.386 - mean_eps: 0.420 - ale.lives: 2.783\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0027\n",
      "19 episodes - episode_reward: 1.158 [0.000, 4.000] - loss: 0.005 - mae: 1.284 - mean_q: 1.409 - mean_eps: 0.411 - ale.lives: 2.571\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0114\n",
      "7 episodes - episode_reward: 16.571 [9.000, 27.000] - loss: 0.005 - mae: 1.300 - mean_q: 1.424 - mean_eps: 0.402 - ale.lives: 2.630\n",
      "\n",
      "Interval 68 (670000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 215s 21ms/step - reward: 0.0051\n",
      "17 episodes - episode_reward: 2.471 [0.000, 7.000] - loss: 0.005 - mae: 1.324 - mean_q: 1.450 - mean_eps: 0.393 - ale.lives: 2.416\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: 0.0113\n",
      "6 episodes - episode_reward: 18.667 [11.000, 31.000] - loss: 0.005 - mae: 1.347 - mean_q: 1.471 - mean_eps: 0.384 - ale.lives: 2.495\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 208s 21ms/step - reward: 0.0121\n",
      "8 episodes - episode_reward: 16.625 [6.000, 30.000] - loss: 0.006 - mae: 1.370 - mean_q: 1.495 - mean_eps: 0.375 - ale.lives: 2.462\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0065\n",
      "16 episodes - episode_reward: 4.000 [0.000, 12.000] - loss: 0.006 - mae: 1.389 - mean_q: 1.516 - mean_eps: 0.366 - ale.lives: 2.607\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0070\n",
      "15 episodes - episode_reward: 4.667 [0.000, 14.000] - loss: 0.005 - mae: 1.390 - mean_q: 1.516 - mean_eps: 0.357 - ale.lives: 2.583\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0103\n",
      "8 episodes - episode_reward: 12.250 [4.000, 25.000] - loss: 0.005 - mae: 1.420 - mean_q: 1.548 - mean_eps: 0.348 - ale.lives: 2.538\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0062\n",
      "15 episodes - episode_reward: 4.600 [0.000, 15.000] - loss: 0.006 - mae: 1.441 - mean_q: 1.569 - mean_eps: 0.339 - ale.lives: 2.471\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0110\n",
      "9 episodes - episode_reward: 12.222 [3.000, 24.000] - loss: 0.006 - mae: 1.477 - mean_q: 1.610 - mean_eps: 0.330 - ale.lives: 2.323\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0065\n",
      "15 episodes - episode_reward: 4.133 [0.000, 9.000] - loss: 0.006 - mae: 1.506 - mean_q: 1.640 - mean_eps: 0.321 - ale.lives: 2.459\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0046\n",
      "17 episodes - episode_reward: 2.529 [0.000, 6.000] - loss: 0.006 - mae: 1.532 - mean_q: 1.668 - mean_eps: 0.312 - ale.lives: 2.630\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: 0.0111\n",
      "8 episodes - episode_reward: 14.375 [6.000, 22.000] - loss: 0.006 - mae: 1.567 - mean_q: 1.703 - mean_eps: 0.303 - ale.lives: 2.448\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0097\n",
      "10 episodes - episode_reward: 9.900 [4.000, 17.000] - loss: 0.006 - mae: 1.584 - mean_q: 1.721 - mean_eps: 0.294 - ale.lives: 2.486\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0110\n",
      "10 episodes - episode_reward: 11.000 [0.000, 21.000] - loss: 0.006 - mae: 1.588 - mean_q: 1.724 - mean_eps: 0.285 - ale.lives: 2.562\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0041\n",
      "18 episodes - episode_reward: 2.222 [0.000, 9.000] - loss: 0.006 - mae: 1.606 - mean_q: 1.743 - mean_eps: 0.276 - ale.lives: 2.445\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0103\n",
      "9 episodes - episode_reward: 9.889 [6.000, 14.000] - loss: 0.006 - mae: 1.613 - mean_q: 1.750 - mean_eps: 0.267 - ale.lives: 2.446\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0126\n",
      "7 episodes - episode_reward: 18.429 [9.000, 27.000] - loss: 0.006 - mae: 1.636 - mean_q: 1.776 - mean_eps: 0.258 - ale.lives: 2.660\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0110\n",
      "8 episodes - episode_reward: 13.750 [4.000, 20.000] - loss: 0.006 - mae: 1.634 - mean_q: 1.773 - mean_eps: 0.249 - ale.lives: 2.361\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0112\n",
      "8 episodes - episode_reward: 15.500 [8.000, 33.000] - loss: 0.006 - mae: 1.643 - mean_q: 1.780 - mean_eps: 0.240 - ale.lives: 2.634\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0129\n",
      "6 episodes - episode_reward: 19.333 [13.000, 31.000] - loss: 0.006 - mae: 1.653 - mean_q: 1.790 - mean_eps: 0.231 - ale.lives: 2.562\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 210s 21ms/step - reward: 0.0129\n",
      "8 episodes - episode_reward: 17.750 [15.000, 21.000] - loss: 0.006 - mae: 1.655 - mean_q: 1.793 - mean_eps: 0.222 - ale.lives: 2.613\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 209s 21ms/step - reward: 0.0107\n",
      "8 episodes - episode_reward: 13.375 [7.000, 23.000] - loss: 0.006 - mae: 1.685 - mean_q: 1.825 - mean_eps: 0.213 - ale.lives: 2.618\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0069\n",
      "13 episodes - episode_reward: 4.923 [1.000, 9.000] - loss: 0.006 - mae: 1.699 - mean_q: 1.840 - mean_eps: 0.204 - ale.lives: 2.527\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 211s 21ms/step - reward: 0.0130\n",
      "7 episodes - episode_reward: 17.000 [10.000, 23.000] - loss: 0.006 - mae: 1.716 - mean_q: 1.862 - mean_eps: 0.195 - ale.lives: 2.488\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 212s 21ms/step - reward: 0.0121\n",
      "8 episodes - episode_reward: 16.500 [8.000, 21.000] - loss: 0.006 - mae: 1.732 - mean_q: 1.877 - mean_eps: 0.186 - ale.lives: 2.693\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 213s 21ms/step - reward: 0.0071\n",
      "13 episodes - episode_reward: 5.769 [3.000, 9.000] - loss: 0.006 - mae: 1.758 - mean_q: 1.907 - mean_eps: 0.177 - ale.lives: 2.511\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0095\n",
      "12 episodes - episode_reward: 7.583 [2.000, 21.000] - loss: 0.006 - mae: 1.760 - mean_q: 1.907 - mean_eps: 0.168 - ale.lives: 2.637\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 214s 21ms/step - reward: 0.0127\n",
      "8 episodes - episode_reward: 15.375 [8.000, 24.000] - loss: 0.006 - mae: 1.776 - mean_q: 1.923 - mean_eps: 0.159 - ale.lives: 2.490\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0080\n",
      "14 episodes - episode_reward: 5.929 [0.000, 12.000] - loss: 0.006 - mae: 1.800 - mean_q: 1.947 - mean_eps: 0.150 - ale.lives: 2.407\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 216s 22ms/step - reward: 0.0108\n",
      "11 episodes - episode_reward: 9.727 [4.000, 14.000] - loss: 0.006 - mae: 1.795 - mean_q: 1.942 - mean_eps: 0.141 - ale.lives: 2.614\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 215s 22ms/step - reward: 0.0121\n",
      "10 episodes - episode_reward: 12.700 [4.000, 21.000] - loss: 0.007 - mae: 1.816 - mean_q: 1.966 - mean_eps: 0.132 - ale.lives: 2.463\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0094\n",
      "12 episodes - episode_reward: 7.667 [3.000, 17.000] - loss: 0.006 - mae: 1.817 - mean_q: 1.964 - mean_eps: 0.123 - ale.lives: 2.508\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0112\n",
      "7 episodes - episode_reward: 16.143 [13.000, 22.000] - loss: 0.006 - mae: 1.836 - mean_q: 1.985 - mean_eps: 0.114 - ale.lives: 2.609\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0116\n",
      "7 episodes - episode_reward: 14.286 [7.000, 24.000] - loss: 0.006 - mae: 1.855 - mean_q: 2.006 - mean_eps: 0.105 - ale.lives: 2.672\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0126\n",
      "7 episodes - episode_reward: 18.143 [10.000, 24.000] - loss: 0.006 - mae: 1.866 - mean_q: 2.016 - mean_eps: 0.100 - ale.lives: 2.629\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 217s 22ms/step - reward: 0.0136\n",
      "5 episodes - episode_reward: 27.000 [13.000, 38.000] - loss: 0.006 - mae: 1.885 - mean_q: 2.035 - mean_eps: 0.100 - ale.lives: 2.579\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0106\n",
      "8 episodes - episode_reward: 14.375 [11.000, 22.000] - loss: 0.006 - mae: 1.897 - mean_q: 2.047 - mean_eps: 0.100 - ale.lives: 2.798\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 218s 22ms/step - reward: 0.0115\n",
      "7 episodes - episode_reward: 15.286 [13.000, 19.000] - loss: 0.006 - mae: 1.880 - mean_q: 2.029 - mean_eps: 0.100 - ale.lives: 2.290\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0120\n",
      "8 episodes - episode_reward: 16.875 [10.000, 25.000] - loss: 0.006 - mae: 1.890 - mean_q: 2.040 - mean_eps: 0.100 - ale.lives: 2.210\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0116\n",
      "7 episodes - episode_reward: 16.857 [10.000, 26.000] - loss: 0.006 - mae: 1.898 - mean_q: 2.048 - mean_eps: 0.100 - ale.lives: 2.377\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0117\n",
      "6 episodes - episode_reward: 19.167 [12.000, 34.000] - loss: 0.006 - mae: 1.923 - mean_q: 2.076 - mean_eps: 0.100 - ale.lives: 2.596\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 219s 22ms/step - reward: 0.0119\n",
      "6 episodes - episode_reward: 17.667 [11.000, 21.000] - loss: 0.006 - mae: 1.932 - mean_q: 2.086 - mean_eps: 0.100 - ale.lives: 2.595\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0120\n",
      "6 episodes - episode_reward: 20.000 [10.000, 33.000] - loss: 0.006 - mae: 1.949 - mean_q: 2.103 - mean_eps: 0.100 - ale.lives: 2.374\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0111\n",
      "7 episodes - episode_reward: 18.000 [11.000, 26.000] - loss: 0.006 - mae: 1.947 - mean_q: 2.100 - mean_eps: 0.100 - ale.lives: 2.510\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0123\n",
      "7 episodes - episode_reward: 16.143 [13.000, 23.000] - loss: 0.006 - mae: 1.972 - mean_q: 2.126 - mean_eps: 0.100 - ale.lives: 2.488\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0109\n",
      "8 episodes - episode_reward: 14.000 [10.000, 19.000] - loss: 0.006 - mae: 1.981 - mean_q: 2.135 - mean_eps: 0.100 - ale.lives: 2.220\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 220s 22ms/step - reward: 0.0117\n",
      "7 episodes - episode_reward: 16.286 [9.000, 23.000] - loss: 0.006 - mae: 1.996 - mean_q: 2.151 - mean_eps: 0.100 - ale.lives: 2.500\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0116\n",
      "8 episodes - episode_reward: 15.000 [7.000, 19.000] - loss: 0.006 - mae: 2.019 - mean_q: 2.175 - mean_eps: 0.100 - ale.lives: 2.450\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0116\n",
      "6 episodes - episode_reward: 17.167 [12.000, 29.000] - loss: 0.006 - mae: 2.028 - mean_q: 2.186 - mean_eps: 0.100 - ale.lives: 2.469\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0132\n",
      "7 episodes - episode_reward: 21.143 [12.000, 30.000] - loss: 0.006 - mae: 2.030 - mean_q: 2.188 - mean_eps: 0.100 - ale.lives: 2.707\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0114\n",
      "7 episodes - episode_reward: 16.000 [8.000, 23.000] - loss: 0.005 - mae: 2.046 - mean_q: 2.205 - mean_eps: 0.100 - ale.lives: 2.402\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0117\n",
      "7 episodes - episode_reward: 14.857 [7.000, 29.000] - loss: 0.006 - mae: 2.051 - mean_q: 2.210 - mean_eps: 0.100 - ale.lives: 2.446\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0116\n",
      "7 episodes - episode_reward: 18.857 [12.000, 24.000] - loss: 0.006 - mae: 2.040 - mean_q: 2.197 - mean_eps: 0.100 - ale.lives: 2.544\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0120\n",
      "7 episodes - episode_reward: 16.714 [12.000, 30.000] - loss: 0.006 - mae: 2.055 - mean_q: 2.214 - mean_eps: 0.100 - ale.lives: 2.604\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0126\n",
      "7 episodes - episode_reward: 17.286 [14.000, 22.000] - loss: 0.005 - mae: 2.068 - mean_q: 2.227 - mean_eps: 0.100 - ale.lives: 2.623\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0102\n",
      "6 episodes - episode_reward: 16.000 [7.000, 24.000] - loss: 0.006 - mae: 2.075 - mean_q: 2.233 - mean_eps: 0.100 - ale.lives: 2.603\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0121\n",
      "8 episodes - episode_reward: 17.125 [11.000, 22.000] - loss: 0.005 - mae: 2.092 - mean_q: 2.250 - mean_eps: 0.100 - ale.lives: 2.427\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0116\n",
      "6 episodes - episode_reward: 18.167 [7.000, 25.000] - loss: 0.005 - mae: 2.098 - mean_q: 2.258 - mean_eps: 0.100 - ale.lives: 2.616\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0118\n",
      "6 episodes - episode_reward: 18.667 [13.000, 25.000] - loss: 0.005 - mae: 2.105 - mean_q: 2.265 - mean_eps: 0.100 - ale.lives: 2.580\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0134\n",
      "5 episodes - episode_reward: 27.400 [20.000, 35.000] - loss: 0.005 - mae: 2.118 - mean_q: 2.278 - mean_eps: 0.100 - ale.lives: 2.574\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0133\n",
      "6 episodes - episode_reward: 22.000 [14.000, 35.000] - loss: 0.006 - mae: 2.109 - mean_q: 2.271 - mean_eps: 0.100 - ale.lives: 2.483\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0118\n",
      "7 episodes - episode_reward: 17.286 [8.000, 29.000] - loss: 0.005 - mae: 2.120 - mean_q: 2.281 - mean_eps: 0.100 - ale.lives: 2.555\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0126\n",
      "7 episodes - episode_reward: 18.000 [13.000, 22.000] - loss: 0.005 - mae: 2.122 - mean_q: 2.283 - mean_eps: 0.100 - ale.lives: 2.416\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0119\n",
      "7 episodes - episode_reward: 17.429 [12.000, 23.000] - loss: 0.005 - mae: 2.123 - mean_q: 2.283 - mean_eps: 0.100 - ale.lives: 2.574\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0118\n",
      "7 episodes - episode_reward: 17.571 [11.000, 22.000] - loss: 0.005 - mae: 2.124 - mean_q: 2.284 - mean_eps: 0.100 - ale.lives: 2.586\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0124\n",
      "6 episodes - episode_reward: 19.667 [9.000, 33.000] - loss: 0.005 - mae: 2.131 - mean_q: 2.292 - mean_eps: 0.100 - ale.lives: 2.636\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0104\n",
      "8 episodes - episode_reward: 13.125 [8.000, 22.000] - loss: 0.005 - mae: 2.130 - mean_q: 2.289 - mean_eps: 0.100 - ale.lives: 2.343\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0120\n",
      "7 episodes - episode_reward: 17.000 [8.000, 24.000] - loss: 0.005 - mae: 2.120 - mean_q: 2.279 - mean_eps: 0.100 - ale.lives: 2.540\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0106\n",
      "8 episodes - episode_reward: 13.625 [4.000, 19.000] - loss: 0.005 - mae: 2.134 - mean_q: 2.295 - mean_eps: 0.100 - ale.lives: 2.576\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0132\n",
      "7 episodes - episode_reward: 16.857 [11.000, 34.000] - loss: 0.005 - mae: 2.143 - mean_q: 2.306 - mean_eps: 0.100 - ale.lives: 2.393\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0117\n",
      "7 episodes - episode_reward: 16.286 [8.000, 27.000] - loss: 0.005 - mae: 2.154 - mean_q: 2.317 - mean_eps: 0.100 - ale.lives: 2.479\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0132\n",
      "9 episodes - episode_reward: 16.444 [13.000, 21.000] - loss: 0.005 - mae: 2.149 - mean_q: 2.311 - mean_eps: 0.100 - ale.lives: 2.357\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0124\n",
      "6 episodes - episode_reward: 18.833 [13.000, 24.000] - loss: 0.005 - mae: 2.154 - mean_q: 2.316 - mean_eps: 0.100 - ale.lives: 2.577\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0122\n",
      "8 episodes - episode_reward: 16.000 [7.000, 29.000] - loss: 0.005 - mae: 2.164 - mean_q: 2.326 - mean_eps: 0.100 - ale.lives: 2.607\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0133\n",
      "7 episodes - episode_reward: 19.429 [12.000, 32.000] - loss: 0.005 - mae: 2.170 - mean_q: 2.333 - mean_eps: 0.100 - ale.lives: 2.475\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0130\n",
      "7 episodes - episode_reward: 17.857 [11.000, 26.000] - loss: 0.005 - mae: 2.171 - mean_q: 2.334 - mean_eps: 0.100 - ale.lives: 2.562\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0120\n",
      "7 episodes - episode_reward: 17.286 [6.000, 21.000] - loss: 0.005 - mae: 2.155 - mean_q: 2.318 - mean_eps: 0.100 - ale.lives: 2.682\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0133\n",
      "7 episodes - episode_reward: 20.429 [11.000, 30.000] - loss: 0.005 - mae: 2.168 - mean_q: 2.331 - mean_eps: 0.100 - ale.lives: 2.387\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0142\n",
      "6 episodes - episode_reward: 23.667 [14.000, 33.000] - loss: 0.005 - mae: 2.173 - mean_q: 2.337 - mean_eps: 0.100 - ale.lives: 2.476\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0148\n",
      "6 episodes - episode_reward: 22.833 [15.000, 44.000] - loss: 0.005 - mae: 2.181 - mean_q: 2.345 - mean_eps: 0.100 - ale.lives: 2.474\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0121\n",
      "7 episodes - episode_reward: 17.714 [9.000, 24.000] - loss: 0.005 - mae: 2.191 - mean_q: 2.354 - mean_eps: 0.100 - ale.lives: 2.525\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0123\n",
      "8 episodes - episode_reward: 16.375 [5.000, 24.000] - loss: 0.005 - mae: 2.172 - mean_q: 2.335 - mean_eps: 0.100 - ale.lives: 2.577\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0134\n",
      "6 episodes - episode_reward: 20.500 [13.000, 28.000] - loss: 0.005 - mae: 2.173 - mean_q: 2.336 - mean_eps: 0.100 - ale.lives: 2.459\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0131\n",
      "7 episodes - episode_reward: 18.143 [10.000, 34.000] - loss: 0.005 - mae: 2.188 - mean_q: 2.352 - mean_eps: 0.100 - ale.lives: 2.449\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0127\n",
      "6 episodes - episode_reward: 19.167 [10.000, 29.000] - loss: 0.005 - mae: 2.187 - mean_q: 2.352 - mean_eps: 0.100 - ale.lives: 2.330\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0137\n",
      "9 episodes - episode_reward: 16.444 [8.000, 27.000] - loss: 0.005 - mae: 2.192 - mean_q: 2.355 - mean_eps: 0.100 - ale.lives: 2.503\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0132\n",
      "7 episodes - episode_reward: 19.143 [13.000, 32.000] - loss: 0.005 - mae: 2.195 - mean_q: 2.358 - mean_eps: 0.100 - ale.lives: 2.562\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0145\n",
      "6 episodes - episode_reward: 23.833 [8.000, 35.000] - loss: 0.005 - mae: 2.192 - mean_q: 2.355 - mean_eps: 0.100 - ale.lives: 2.557\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0128\n",
      "6 episodes - episode_reward: 23.000 [13.000, 35.000] - loss: 0.005 - mae: 2.209 - mean_q: 2.375 - mean_eps: 0.100 - ale.lives: 2.440\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0138\n",
      "7 episodes - episode_reward: 16.571 [10.000, 23.000] - loss: 0.005 - mae: 2.213 - mean_q: 2.379 - mean_eps: 0.100 - ale.lives: 2.257\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0120\n",
      "8 episodes - episode_reward: 17.500 [7.000, 29.000] - loss: 0.005 - mae: 2.202 - mean_q: 2.368 - mean_eps: 0.100 - ale.lives: 2.526\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0136\n",
      "6 episodes - episode_reward: 21.000 [16.000, 26.000] - loss: 0.005 - mae: 2.202 - mean_q: 2.367 - mean_eps: 0.100 - ale.lives: 2.527\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0153\n",
      "6 episodes - episode_reward: 25.667 [18.000, 33.000] - loss: 0.005 - mae: 2.201 - mean_q: 2.366 - mean_eps: 0.100 - ale.lives: 2.750\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0161\n",
      "6 episodes - episode_reward: 27.167 [9.000, 43.000] - loss: 0.005 - mae: 2.210 - mean_q: 2.374 - mean_eps: 0.100 - ale.lives: 2.571\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0141\n",
      "7 episodes - episode_reward: 21.714 [13.000, 33.000] - loss: 0.005 - mae: 2.212 - mean_q: 2.378 - mean_eps: 0.100 - ale.lives: 2.587\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0152\n",
      "6 episodes - episode_reward: 21.667 [13.000, 30.000] - loss: 0.005 - mae: 2.226 - mean_q: 2.393 - mean_eps: 0.100 - ale.lives: 2.349\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0153\n",
      "6 episodes - episode_reward: 28.833 [24.000, 39.000] - loss: 0.005 - mae: 2.215 - mean_q: 2.381 - mean_eps: 0.100 - ale.lives: 2.389\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0152\n",
      "5 episodes - episode_reward: 28.000 [17.000, 35.000] - loss: 0.005 - mae: 2.216 - mean_q: 2.383 - mean_eps: 0.100 - ale.lives: 2.474\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0158\n",
      "7 episodes - episode_reward: 23.286 [15.000, 29.000] - loss: 0.005 - mae: 2.228 - mean_q: 2.395 - mean_eps: 0.100 - ale.lives: 2.425\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0147\n",
      "6 episodes - episode_reward: 22.333 [17.000, 34.000] - loss: 0.005 - mae: 2.232 - mean_q: 2.398 - mean_eps: 0.100 - ale.lives: 2.433\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0152\n",
      "6 episodes - episode_reward: 27.167 [20.000, 36.000] - loss: 0.005 - mae: 2.227 - mean_q: 2.394 - mean_eps: 0.100 - ale.lives: 2.520\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0152\n",
      "5 episodes - episode_reward: 30.400 [20.000, 37.000] - loss: 0.005 - mae: 2.224 - mean_q: 2.390 - mean_eps: 0.100 - ale.lives: 2.536\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0164\n",
      "7 episodes - episode_reward: 25.571 [15.000, 37.000] - loss: 0.005 - mae: 2.223 - mean_q: 2.390 - mean_eps: 0.100 - ale.lives: 2.458\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0152\n",
      "6 episodes - episode_reward: 24.333 [16.000, 40.000] - loss: 0.005 - mae: 2.229 - mean_q: 2.394 - mean_eps: 0.100 - ale.lives: 2.364\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0163\n",
      "6 episodes - episode_reward: 28.000 [18.000, 40.000] - loss: 0.005 - mae: 2.228 - mean_q: 2.394 - mean_eps: 0.100 - ale.lives: 2.576\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0177\n",
      "4 episodes - episode_reward: 38.000 [30.000, 53.000] - loss: 0.005 - mae: 2.232 - mean_q: 2.399 - mean_eps: 0.100 - ale.lives: 2.548\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0163\n",
      "6 episodes - episode_reward: 31.000 [21.000, 41.000] - loss: 0.005 - mae: 2.227 - mean_q: 2.394 - mean_eps: 0.100 - ale.lives: 2.259\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0153\n",
      "6 episodes - episode_reward: 22.500 [14.000, 37.000] - loss: 0.005 - mae: 2.233 - mean_q: 2.401 - mean_eps: 0.100 - ale.lives: 2.686\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0153\n",
      "6 episodes - episode_reward: 28.667 [14.000, 43.000] - loss: 0.005 - mae: 2.239 - mean_q: 2.406 - mean_eps: 0.100 - ale.lives: 2.337\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0159\n",
      "6 episodes - episode_reward: 26.167 [18.000, 39.000] - loss: 0.005 - mae: 2.229 - mean_q: 2.396 - mean_eps: 0.100 - ale.lives: 2.515\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0144\n",
      "6 episodes - episode_reward: 22.333 [14.000, 35.000] - loss: 0.005 - mae: 2.245 - mean_q: 2.413 - mean_eps: 0.100 - ale.lives: 2.496\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0160\n",
      "7 episodes - episode_reward: 24.429 [6.000, 37.000] - loss: 0.005 - mae: 2.241 - mean_q: 2.410 - mean_eps: 0.100 - ale.lives: 2.589\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0152\n",
      "6 episodes - episode_reward: 25.500 [10.000, 35.000] - loss: 0.005 - mae: 2.246 - mean_q: 2.414 - mean_eps: 0.100 - ale.lives: 2.561\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0170\n",
      "6 episodes - episode_reward: 27.000 [16.000, 43.000] - loss: 0.005 - mae: 2.250 - mean_q: 2.418 - mean_eps: 0.100 - ale.lives: 2.353\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0166\n",
      "6 episodes - episode_reward: 29.000 [23.000, 41.000] - loss: 0.005 - mae: 2.250 - mean_q: 2.418 - mean_eps: 0.100 - ale.lives: 2.639\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0186\n",
      "4 episodes - episode_reward: 42.250 [32.000, 49.000] - loss: 0.005 - mae: 2.251 - mean_q: 2.420 - mean_eps: 0.100 - ale.lives: 2.634\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0174\n",
      "5 episodes - episode_reward: 34.200 [18.000, 50.000] - loss: 0.005 - mae: 2.258 - mean_q: 2.428 - mean_eps: 0.100 - ale.lives: 2.477\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0167\n",
      "6 episodes - episode_reward: 27.167 [10.000, 44.000] - loss: 0.005 - mae: 2.260 - mean_q: 2.431 - mean_eps: 0.100 - ale.lives: 2.694\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0183\n",
      "6 episodes - episode_reward: 34.833 [26.000, 45.000] - loss: 0.005 - mae: 2.263 - mean_q: 2.434 - mean_eps: 0.100 - ale.lives: 2.587\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0175\n",
      "4 episodes - episode_reward: 32.000 [13.000, 51.000] - loss: 0.005 - mae: 2.278 - mean_q: 2.450 - mean_eps: 0.100 - ale.lives: 2.544\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0173\n",
      "6 episodes - episode_reward: 28.333 [12.000, 50.000] - loss: 0.005 - mae: 2.284 - mean_q: 2.457 - mean_eps: 0.100 - ale.lives: 2.456\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0183\n",
      "7 episodes - episode_reward: 33.143 [18.000, 51.000] - loss: 0.005 - mae: 2.288 - mean_q: 2.461 - mean_eps: 0.100 - ale.lives: 2.486\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0177\n",
      "5 episodes - episode_reward: 32.800 [22.000, 44.000] - loss: 0.005 - mae: 2.312 - mean_q: 2.486 - mean_eps: 0.100 - ale.lives: 2.631\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0167\n",
      "5 episodes - episode_reward: 34.800 [21.000, 48.000] - loss: 0.005 - mae: 2.306 - mean_q: 2.480 - mean_eps: 0.100 - ale.lives: 2.644\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0185\n",
      "6 episodes - episode_reward: 30.000 [18.000, 38.000] - loss: 0.005 - mae: 2.313 - mean_q: 2.487 - mean_eps: 0.100 - ale.lives: 2.311\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0182\n",
      "4 episodes - episode_reward: 38.000 [31.000, 46.000] - loss: 0.005 - mae: 2.319 - mean_q: 2.493 - mean_eps: 0.100 - ale.lives: 2.459\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0169\n",
      "6 episodes - episode_reward: 32.167 [16.000, 57.000] - loss: 0.005 - mae: 2.314 - mean_q: 2.487 - mean_eps: 0.100 - ale.lives: 2.511\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0184\n",
      "7 episodes - episode_reward: 28.000 [13.000, 42.000] - loss: 0.005 - mae: 2.315 - mean_q: 2.490 - mean_eps: 0.100 - ale.lives: 2.717\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0185\n",
      "5 episodes - episode_reward: 34.400 [16.000, 46.000] - loss: 0.005 - mae: 2.321 - mean_q: 2.496 - mean_eps: 0.100 - ale.lives: 2.410\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 221s 22ms/step - reward: 0.0200\n",
      "5 episodes - episode_reward: 42.200 [37.000, 45.000] - loss: 0.005 - mae: 2.319 - mean_q: 2.495 - mean_eps: 0.100 - ale.lives: 2.631\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0188\n",
      "5 episodes - episode_reward: 32.200 [21.000, 48.000] - loss: 0.005 - mae: 2.331 - mean_q: 2.508 - mean_eps: 0.100 - ale.lives: 2.578\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0204\n",
      "6 episodes - episode_reward: 38.833 [24.000, 58.000] - loss: 0.005 - mae: 2.338 - mean_q: 2.516 - mean_eps: 0.100 - ale.lives: 2.615\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0197\n",
      "6 episodes - episode_reward: 32.500 [12.000, 61.000] - loss: 0.005 - mae: 2.338 - mean_q: 2.514 - mean_eps: 0.100 - ale.lives: 2.736\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0197\n",
      "done, took 43384.642 seconds\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "#Start Training\n",
    "if TRANSFER_MODE:\n",
    "    #Found a similar game\n",
    "    print(\"Transfer Learning\")\n",
    "    trainAgent.trainingInjection()\n",
    "else:\n",
    "    #did not find a similar game\n",
    "    print(\"Normal Learning\")\n",
    "    trainAgent.trainingBaseline()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (impl)",
   "language": "python",
   "name": "impl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
