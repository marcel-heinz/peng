{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T16:27:16.164279Z",
     "start_time": "2020-03-09T16:27:07.663267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cbrUtil.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import (Activation, \n",
    "                          Convolution2D, \n",
    "                          Dense, \n",
    "                          Flatten, \n",
    "                          Permute)\n",
    "from keras.models import (Sequential, \n",
    "                          load_model)\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import (FileLogger, \n",
    "                          ModelIntervalCheckpoint, \n",
    "                          WandbLogger)\n",
    "from rl.core import Processor\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import (EpsGreedyQPolicy, \n",
    "                       LinearAnnealedPolicy)\n",
    "\n",
    "import import_ipynb\n",
    "from cbrUtil import AtariProcessor\n",
    "import timeout_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cb/most similar case and query case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T16:16:14.662522Z",
     "start_time": "2020-03-07T16:16:14.655607Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'Alien-v0'\n",
    "mostSimCase = {'Seaquest-v0': 0.5}\n",
    "caseBase = {'Assault-v0': 7, 'BreakoutDeterministic-v4': 4, 'MsPacman-v0': 9, 'Seaquest-v0': 18, 'SpaceInvaders-v0': 6}\n",
    "queryCase = {'Alien-v0': 18}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cbrTest\n",
    "\n",
    "- [x] check if nb_actions are same\n",
    "    - if same\n",
    "        - [x] do nothing\n",
    "    - if not same\n",
    "        - [x] change last layer to target network and init last layer weights\n",
    "- [x] plug-in all policies from most similar case and determine highest reward policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T16:27:44.519665Z",
     "start_time": "2020-03-09T16:27:44.465837Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestAgent():\n",
    "    \"\"\"\n",
    "    This class implements the \"Training Agent\" of the proposed PENG \n",
    "    Architecture.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    env_name : str\n",
    "        exact name of the Atari environment.\n",
    "    \n",
    "    mostSimCase : dict\n",
    "        name of the most similar case with similarity\n",
    "        \n",
    "    caseBase : dict\n",
    "        current model repository\n",
    "        \n",
    "    queryCase : dict\n",
    "        current target gameplay task with number of actions\n",
    "    \n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    loadSimArchitecture(self):\n",
    "        loads current architecture of most similar case.\n",
    "    \n",
    "    getTestingCase(self):\n",
    "        loads current testing case.\n",
    "    \n",
    "    checkActionSize(self):\n",
    "        checks if action space is equal.\n",
    "    \n",
    "    testChecker(self):\n",
    "        builds DRL agent and checks env.\n",
    "    \n",
    "    testAgent(self, dqn ,verbose=1):\n",
    "        runs environment for one episode in order to get reward.\n",
    "    \n",
    "    getTrainPolicy(self):\n",
    "        returns policy for the training agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 mostSimCase, \n",
    "                 caseBase, \n",
    "                 queryCase):\n",
    "        \"\"\"\n",
    "        Initializes the agent.\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        env_name : str\n",
    "            exact name of the Atari environment.\n",
    "\n",
    "        mostSimCase : dict\n",
    "            name of the most similar case with similarity\n",
    "\n",
    "        caseBase : dict\n",
    "            current model repository\n",
    "\n",
    "        queryCase : dict\n",
    "            current target gameplay task with number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(mostSimCase)==0:\n",
    "            print(\"No similar case...\")\n",
    "            self.transferMode = False\n",
    "        \n",
    "        else:\n",
    "            self.TIME = str(int(time.time()))\n",
    "            self.INPUT_SHAPE = (84,84)\n",
    "            self.WINDOW_LENGTH = 4\n",
    "            self.transferMode = True\n",
    "            self.mostSimCase = mostSimCase\n",
    "            self.caseBase = caseBase\n",
    "            self.queryCase = queryCase\n",
    "\n",
    "            self.query_environment = None\n",
    "            self.testing_environment = None\n",
    "            \n",
    "\n",
    "            self.path_to_games = './KC/ModelRepo/Atari_'\n",
    "\n",
    "            self.path_to_architecture = '/Architecture/'\n",
    "            self.path_to_policy = '/Policy/'\n",
    "\n",
    "            self.testingCase = self.getTestingCase()\n",
    "            self.testingCaseActions = None\n",
    "            self.same_action_size = self.checkActionSize()\n",
    "            \n",
    "            self.env_name = self.query_environment\n",
    "            self.env = gym.make(self.env_name)\n",
    "            np.random.seed(123)\n",
    "            self.env.seed(123)\n",
    "            #get nb_actions for the learning env\n",
    "            self.nb_actions = self.env.action_space.n\n",
    "            \n",
    "            self.model = None\n",
    "            self.memory = None\n",
    "            self.processor = None\n",
    "            self.policy = None\n",
    "            self.dqn = None\n",
    "            self.history = None\n",
    "            \n",
    "            self.similar_architecture_path = (self.path_to_games+\n",
    "                                        self.testing_environment+\n",
    "                                        self.path_to_architecture)\n",
    "            \n",
    "            self.similar_policy_path = (self.path_to_games+\n",
    "                                        self.testing_environment+\n",
    "                                        self.path_to_policy)\n",
    "            \n",
    "            self.similar_architecture = self.loadSimArchitecture()\n",
    "            \n",
    "\n",
    "            self.testedPolicies = self.testChecker()\n",
    "            self.trainPolicy = self.getTrainPolicy()\n",
    "        \n",
    "    def loadSimArchitecture(self):\n",
    "        \"\"\"\n",
    "        loads current architecture of most similar case.\n",
    "        \"\"\"\n",
    "        archi = None\n",
    "        for architecture in os.listdir(self.similar_architecture_path):\n",
    "            if 'h5' in architecture:\n",
    "                archi = load_model(self.similar_architecture_path+\n",
    "                                   architecture)\n",
    "        return archi\n",
    "        \n",
    "        \n",
    "    def getTestingCase(self):\n",
    "        \"\"\"\n",
    "        loads current testing case.\n",
    "        \"\"\"\n",
    "        testingCase = {}\n",
    "        for case in self.mostSimCase:\n",
    "            self.testing_environment = case\n",
    "            testingCase[case] = self.caseBase[case]\n",
    "            break\n",
    "        return testingCase\n",
    "        \n",
    "    def checkActionSize(self):\n",
    "        \"\"\"\n",
    "        checks if action space is equal.\n",
    "        \"\"\"\n",
    "        action_size = {}\n",
    "        env_query = None\n",
    "        for game in self.queryCase:\n",
    "            env_query = game\n",
    "            self.query_environment = env_query\n",
    "        for game in self.testingCase:\n",
    "            self.testingCaseActions = self.caseBase[game]\n",
    "            action_size[game] = (self.caseBase[game]\n",
    "                                 == self.queryCase[env_query])\n",
    "        return action_size\n",
    "    \n",
    "    def testChecker(self):\n",
    "        \"\"\"\n",
    "        builds DRL agent and checks env.\n",
    "        \"\"\"\n",
    "        for same in self.same_action_size:\n",
    "            if self.same_action_size[same]:\n",
    "                print(\"Nothing to change for architecture\")\n",
    "                testedPolicies = {}\n",
    "                for pol in os.listdir(self.similar_policy_path):\n",
    "                    if 'ipynb' in pol:\n",
    "                        pass\n",
    "                    else:\n",
    "                        file_path = self.similar_policy_path+pol\n",
    "                        newModel = Sequential()\n",
    "                        self.similar_architecture.load_weights(file_path)\n",
    "                        for layer in self.similar_architecture.layers:\n",
    "                            newModel.add(layer)\n",
    "                            \n",
    "                        memory = SequentialMemory(limit=1000000, \n",
    "                                  window_length=self.WINDOW_LENGTH)\n",
    "        \n",
    "                        processor = AtariProcessor()\n",
    "        \n",
    "                        #eps=0.5 worked so far for model injection \n",
    "                        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(action_size=self.nb_actions,\n",
    "                                                            q_Injection = False), \n",
    "                                                      attr='eps', \n",
    "                                                      value_max=1.0, \n",
    "                                                      value_min=.1, \n",
    "                                                      value_test=.05,\n",
    "                                                      nb_steps=1000000)\n",
    "                \n",
    "                        dqn = DQNAgent(model=newModel, \n",
    "                            nb_actions=self.nb_actions, \n",
    "                            policy=policy, \n",
    "                            memory=memory,\n",
    "                            processor=processor, \n",
    "                            nb_steps_warmup=50000, \n",
    "                            gamma=.99, \n",
    "                            target_model_update=10000,\n",
    "                            train_interval=4, delta_clip=1.)\n",
    "        \n",
    "                        dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "                        try:\n",
    "                            hist = self.testAgent(dqn)\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            testedPolicies[pol] = hist.history['episode_reward'][0]\n",
    "                        #print()\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "\n",
    "                testedPolicies={k: v for k, v in sorted(testedPolicies.items(), \n",
    "                                 key=lambda item: item[1],\n",
    "                                 reverse=True)}\n",
    "                return testedPolicies\n",
    "            else:\n",
    "                print(\"Changing last layers for architecture, and testing it.\")\n",
    "                testedPolicies = {}\n",
    "                ite = 0\n",
    "                for pol in os.listdir(self.similar_policy_path):\n",
    "                    if 'ipynb' in pol:\n",
    "                        pass\n",
    "                    else:\n",
    "                        testedPolicies[pol] = {}\n",
    "                        #print(f\"Testing Policy: {pol}\")\n",
    "                        file_path = self.similar_policy_path+pol\n",
    "                        newModel = Sequential()\n",
    "                        self.similar_architecture.load_weights(file_path)\n",
    "                        \n",
    "                        for layer in self.similar_architecture.layers[:-2]:\n",
    "                            newModel.add(layer)\n",
    "                            \n",
    "                        newModel.add(Dense(units=self.nb_actions,\n",
    "                           kernel_initializer='random_uniform',\n",
    "                           name='dense_'+self.TIME))\n",
    "                        \n",
    "                        newModel.add(Activation('linear',\n",
    "                                name='activation_'+self.TIME))\n",
    "                        \n",
    "                        weg = []\n",
    "                        weights = newModel.get_weights()\n",
    "                        for weight in range(len(weights[-1])):\n",
    "                            random_weight = np.random.uniform(low=0.5, high=0.7)\n",
    "                            weg.append(random_weight)\n",
    "                            weights[-1][weight] = random_weight\n",
    "                        newModel.set_weights(weights)\n",
    "                        \n",
    "                        \n",
    "                        memory = SequentialMemory(limit=1000000, \n",
    "                                  window_length=self.WINDOW_LENGTH)\n",
    "        \n",
    "                        processor = AtariProcessor()\n",
    "        \n",
    "                        #eps=0.5 worked so far for model injection \n",
    "                        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(action_size=self.nb_actions,\n",
    "                                                            q_Injection = False), \n",
    "                                                      attr='eps', \n",
    "                                                      value_max=1.0, \n",
    "                                                      value_min=.1, \n",
    "                                                      value_test=.05,\n",
    "                                                      nb_steps=1000000)\n",
    "                \n",
    "                        dqn = DQNAgent(model=newModel, \n",
    "                            nb_actions=self.nb_actions, \n",
    "                            policy=policy, \n",
    "                            memory=memory,\n",
    "                            processor=processor, \n",
    "                            nb_steps_warmup=50000, \n",
    "                            gamma=.99, \n",
    "                            target_model_update=10000,\n",
    "                            train_interval=4, delta_clip=1.)\n",
    "        \n",
    "                        dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
    "                        #hist = None\n",
    "                        try:\n",
    "                            hist = self.testAgent(dqn)\n",
    "                            rew_name = 'reward'+str(ite)\n",
    "                            weg_name = 'weights'+str(ite)\n",
    "                            testedPolicies[rew_name]=hist.history['episode_reward'][0]\n",
    "                            testedPolicies[weg_name]=weg\n",
    "                            ite+=1\n",
    "                        except Exception as e:\n",
    "                            pass \n",
    "                        #print(hist.history['episode_reward'][0])\n",
    "                        try:\n",
    "                            #testedPolicies[pol] = hist.history['episode_reward'][0]\n",
    "                            pass\n",
    "                            #testedPolicies['reward']=hist.history['episode_reward'][0]\n",
    "                            #testedPolicies['weights']=weg\n",
    "                        #print()\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "\n",
    "                #testedPolicies={k: v for k, v in sorted(testedPolicies.items(), \n",
    "                                 #key=lambda item: item[1],\n",
    "                                 #reverse=True)}\n",
    "                return testedPolicies\n",
    "\n",
    "    @timeout_decorator.timeout(10)\n",
    "    def testAgent(self, dqn ,verbose=1):\n",
    "        \"\"\"\n",
    "        runs environment for one episode in order to get reward.\n",
    "        \"\"\"\n",
    "        history = dqn.test(self.env, \n",
    "                          nb_episodes=1, \n",
    "                          visualize=False,\n",
    "                        verbose=verbose) #0 = nothing to show\n",
    "        return history\n",
    "    \n",
    "    def getTrainPolicy(self):\n",
    "        \"\"\"\n",
    "        returns policy for the training agent.\n",
    "        \"\"\"\n",
    "        train_pol = []\n",
    "        for pol in self.testedPolicies:\n",
    "            print(f\"Similar Policy achieved reward of: {self.testedPolicies[pol]}\")\n",
    "            train_pol.append(pol)\n",
    "            break\n",
    "        return train_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-07T16:17:16.292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to change for architecture\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 3.000, steps: 457\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 18.000, steps: 1135\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 690\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 16.000, steps: 821\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 14.000, steps: 1061\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 31.000, steps: 1141\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 455\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 813\n",
      "Testing for 1 episodes ...\n"
     ]
    }
   ],
   "source": [
    "tester = TestAgent(mostSimCase=mostSimCase,\n",
    "                   caseBase=caseBase,\n",
    "                   queryCase=queryCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T10:42:32.425404Z",
     "start_time": "2020-02-03T10:42:32.419170Z"
    }
   },
   "outputs": [],
   "source": [
    "tester.testedPolicies\n",
    "#tester.trainPolicy"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (impl)",
   "language": "python",
   "name": "impl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "336px",
    "left": "976px",
    "right": "20px",
    "top": "120px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
